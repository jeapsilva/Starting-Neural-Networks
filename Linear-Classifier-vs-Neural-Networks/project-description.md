# Project Description

This project demonstrates how a neural network works and when it has advantages compared to a linear classifier. Furthermore, this project shows how the forward and backpropagation steps occur in a neural network problem.

In the forward step, the weight functions start with random values. After that,  information is passed through the neural network. To actualize the neural network in the backpropagation step,  I've used the Cross-Entropy Loss as a loss function, which has been implemented in many algorithms.

In the backpropagation step, was calculated the derivatives formulas to calculate the weights actualization using the descent gradient.

I hope you enjoy this explanation. 
